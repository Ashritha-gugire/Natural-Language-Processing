{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "65be34f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "9cfee20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data, initializing empty lists of train and test data to store once they are read from the files.\n",
    "\n",
    "# Data handling process, loading and preparing dataset \n",
    "data_path = \"C:\\\\Users\\\\ashri\\\\Documents\\\\AIT-526\\\\tweet\\\\tweet\"\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7ac50d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Organizing data for training and testing.\n",
    "## In here filepath is used for joining the base path. Later, it iterates over the two types of sentimetns within each subset.\n",
    "## If statement cehecks if the subset id being processed or not, if so hten it appends the content of file to train_data and it not it appends to test data.\n",
    "\n",
    "for subset in [\"train\", \"test\"]:\n",
    "    subset_path = os.path.join(data_path, subset)\n",
    "    for label in [\"positive\", \"negative\"]:\n",
    "        label_path = os.path.join(subset_path, label)\n",
    "        for filepath in glob(os.path.join(label_path, \"*.txt\")):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                if subset == \"train\":\n",
    "                    train_data.append(text)\n",
    "                    train_labels.append(\"pos\" if label == \"positive\" else \"neg\")\n",
    "                else:\n",
    "                    test_data.append(text)\n",
    "                    test_labels.append(\"pos\" if label == \"positive\" else \"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "67be8b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['@SouthwestAir I would appreciate that.  Thank you.\\n',\n",
       "  '@USAirways thank you very much.\\n',\n",
       "  \"@JetBlue I'm all set. About to fly. Not bad for a first date with a giant metal bird machine. She even brought snacks.\\n\",\n",
       "  '@SouthwestAir I got a flight at 11:55am on Thursday but looking for something tomorrow anything available?\\n',\n",
       "  \"@AmericanAir you're my early frontrunner for best airline! #oscars2016\\n\"],\n",
       " ['pos', 'pos', 'pos', 'pos', 'pos'])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5], train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ee899da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['@SouthwestAir wifi on my plane but I gotta pay for it? Help your broke homegirl out‚úàÔ∏èüì±\\n',\n",
       "  \"@united we're stuck at OGG looks like flight will be Cancelled Flightled. Can you help? =)\\n\",\n",
       "  '@united WTH be honest with your customers.  This better be the last change or we are driving home.  Has our plane left or not!\\n',\n",
       "  '@united Freakin\"\\n'],\n",
       " ['neg', 'neg'])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2000:2004], train_labels[2002:2004]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "038db2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4181\n",
      "@USAirways thank you very much.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "bad00ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from collections import defaultdict\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fae9fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store tweets in a list\n",
    "preprocessed_tweets = []\n",
    "\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(tweet, stem=False, stemmer_type = 'porter'):\n",
    "    # removing HTML tags\n",
    "    soup = BeautifulSoup(tweet, \"html.parser\")\n",
    "    tweet = soup.get_text()\n",
    "    \n",
    "    #lowere case words starting with capital letter\n",
    "    tweet = re.sub(r'\\b([A-Z][a-z]+)\\b', lambda m: m.group(0).lower(), tweet)\n",
    "    \n",
    "    # trabslaye emojis to text\n",
    "    tweet = emoji.demojize(tweet)\n",
    "    \n",
    "    #Tokenize using WordPunctTokenizer to remove the whitespaces\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    # replace unnecessary punctuations with whitespaces\n",
    "    \n",
    "    tweet = re.sub(r'[^\\w\\s]', ' ', tweet)\n",
    "    \n",
    "    #tokens = word_tokenize(sample_tweet)\n",
    "    preprocessed_tweets = ' '.join(tokens)\n",
    "    \n",
    "    #stemming\n",
    "    if stem:\n",
    "        if stemmer_type == 'porter':\n",
    "            stemmer = PorterStemmer()\n",
    "        elif stemmer_type == 'snowball':\n",
    "            stemmer = SnowballStemmer('english')\n",
    "        else:\n",
    "            raise ValueError(\"invalid\")\n",
    "            #tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        preprocessed_tweets = ' '.join([stemmer.stem(token) for token in preprocessed_tweets.split()])\n",
    "    preprocessed_tweet = ' '.join(tokens)\n",
    "    return preprocessed_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7ca266fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashri\\AppData\\Local\\Temp\\ipykernel_13708\\272439863.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(tweet, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Tweet 1: @ southwestair i would appreci that . thank you .\n",
      "\n",
      "Preprocessed Tweet 2: @ usairway thank you veri much .\n",
      "\n",
      "Preprocessed Tweet 3: @ jetblu i ' m all set . about to fli . not bad for a first date with a giant metal bird machin . she even brought snack .\n",
      "\n",
      "Preprocessed Tweet 4: @ southwestair i got a flight at 11 : 55am on thursday but look for someth tomorrow anyth avail ?\n",
      "\n",
      "Preprocessed Tweet 5: @ americanair you ' re my earli frontrunn for best airlin ! # oscars2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## If stem is True then it applies stemming based on the specified type by reducing word to its root form.\n",
    "# Assuming preprocessed_tweets is a list of preprocessed tweet texts\n",
    "preprocessed_tweets = [preprocess(tweet, stem=True) for tweet in train_data]\n",
    "\n",
    "# Print the preprocessed text of the first 5 tweets\n",
    "for i, preprocessed_text in enumerate(preprocessed_tweets[:5]):  # Limiting to first 5 for demonstration\n",
    "    print(f\"Preprocessed Tweet {i+1}: {preprocessed_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "969e82ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_vocabularies(preprocessed_tweets, stemming= False):\n",
    "    positive_words = set()\n",
    "    negative_words = set()\n",
    "\n",
    "    # Loop through each preprocessed tweet\n",
    "    for tweet in preprocessed_tweets:\n",
    "        words = tweet.split()\n",
    "        \n",
    "        # Update positive or negative word sets based on certain keywords\n",
    "        if 'good' in words or 'positive' in words:\n",
    "            positive_words.update(words)\n",
    "        elif 'bad' in words or 'negative' in words:\n",
    "            negative_words.update(words)\n",
    "\n",
    "    # Create vocabularies with binary sentiment values\n",
    "    positive_vocabulary = {word: 1 for word in positive_words}\n",
    "    negative_vocabulary = {word: 0 for word in negative_words}\n",
    "\n",
    "    return positive_vocabulary, negative_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "06ee5d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('agent', 1), ('look', 1), ('garbag', 1), ('sylvi', 1), ('poteettj', 1), ('took', 1), ('have', 1), ('nc0es6e4lf', 1), ('day', 1)]\n",
      "[('agent', 0), ('look', 0), ('bicycl', 0), ('rant', 0), ('tag', 0), ('took', 0), ('day', 0), ('have', 0), ('socialtantrum', 0)]\n"
     ]
    }
   ],
   "source": [
    "# Assuming preprocessed_tweets is defined\n",
    "#preprocessed_tweets = [...]  # Your preprocessed tweets here\n",
    "\n",
    "# Generate positive and negative vocabularies\n",
    "positive_vocabulary, negative_vocabulary = create_vocabularies(preprocessed_tweets, stemming=False)\n",
    "\n",
    "print(list(positive_vocabulary.items())[1:10])\n",
    "\n",
    "print(list(negative_vocabulary.items())[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3ecc4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb(train_data, train_labels):\n",
    "    positive_words = defaultdict(int)\n",
    "    negative_words = defaultdict(int)\n",
    "    n_pos = n_neg = 0\n",
    "\n",
    "    # Instead of using a separate vocabulary creation function,\n",
    "    # directly count word occurrences in positive and negative tweets\n",
    "    for text, label in zip(train_data, train_labels):\n",
    "        tokens = preprocess(text, stem=True)\n",
    "        if label == 'pos':\n",
    "            n_pos += 1\n",
    "            for token in tokens:\n",
    "                positive_words[token] += 1\n",
    "        else:\n",
    "            n_neg += 1\n",
    "            for token in tokens:\n",
    "                negative_words[token] += 1\n",
    "\n",
    "    # Combine positive and negative words to form a complete vocabulary\n",
    "    vocab = set(positive_words.keys()).union(set(negative_words.keys()))\n",
    "    n_total = n_pos + n_neg\n",
    "    prior_pos = n_pos / n_total\n",
    "    prior_neg = n_neg / n_total\n",
    "\n",
    "    # Calculate likelihoods using word counts\n",
    "    pos_likelihoods = {word: (positive_words[word] + 1) / (sum(positive_words.values()) + len(vocab)) for word in vocab}\n",
    "    neg_likelihoods = {word: (negative_words[word] + 1) / (sum(negative_words.values()) + len(vocab)) for word in vocab}\n",
    "    \n",
    "    return vocab, prior_pos, prior_neg, pos_likelihoods, neg_likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3f22c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify text\n",
    "def classify_naive_bayes(text, vocab, p_pos, p_neg, pos_likelihoods, neg_likelihoods):\n",
    "    tokens = preprocess(text, stem=True)\n",
    "    \n",
    "    p_pos_text = prior_pos\n",
    "    p_neg_text = prior_neg\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in vocab:\n",
    "            p_pos_text *= pos_likelihoods[token]\n",
    "            p_neg_text *= neg_likelihoods[token]\n",
    "        else:\n",
    "            # Ignore words not in the vocabulary\n",
    "            pass\n",
    "    \n",
    "    return 'pos' if p_pos_text > p_neg_text else 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "54f5fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(predicted, actual):\n",
    "    confusion_matrix = defaultdict(int)\n",
    "    \n",
    "    for pred, act in zip(predicted, actual):\n",
    "        confusion_matrix[(pred, act)] += 1\n",
    "    \n",
    "    return confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "4edfc9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(confusion_matrix):\n",
    "    total = sum(confusion_matrix.values())\n",
    "    correct = confusion_matrix[('pos', 'pos')] + confusion_matrix[('neg', 'neg')]\n",
    "    return correct / total\n",
    "\n",
    "def calculate_precision(confusion_matrix, class_index):\n",
    "    true_pos = confusion_matrix.get((class_index, class_index), 0)\n",
    "    false_neg = sum(count for (pred, act), count in confusion_matrix.items() if pred != class_index and act == class_index)\n",
    "    return true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "\n",
    "\n",
    "def calculate_recall(confusion_matrix, class_label):\n",
    "    true_pos = confusion_matrix.get((class_label, class_label),0)\n",
    "    false_neg = sum(count for (pred, act), count in confusion_matrix.items() if pred != class_label and act == class_label)\n",
    "    return true_pos / (true_pos + false_neg)\n",
    "\n",
    "def calculate_f1_score(precision, recall):\n",
    "    return 2 * precision * recall / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f626209d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashri\\AppData\\Local\\Temp\\ipykernel_13708\\272439863.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(tweet, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "vocab, prior_pos, prior_neg, pos_likelihoods, neg_likelihoods = train_nb(train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "32eba19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashri\\AppData\\Local\\Temp\\ipykernel_13708\\272439863.py:8: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(tweet, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for text in test_data:\n",
    "    prediction = classify_naive_bayes(text, vocab, prior_pos, prior_neg, pos_likelihoods, neg_likelihoods)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "7a8be363",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = calculate_confusion_matrix(predictions, test_labels)\n",
    "accuracy = calculate_accuracy(confusion_matrix)\n",
    "#print(accuracy)\n",
    "precision_pos = calculate_precision(confusion_matrix, 'pos')\n",
    "precision_neg = calculate_precision(confusion_matrix, 'neg')\n",
    "recall_pos = calculate_recall(confusion_matrix, 'pos')\n",
    "recall_neg = calculate_recall(confusion_matrix, 'neg')\n",
    "f1_pos = calculate_f1_score(precision_pos, recall_pos)\n",
    "f1_neg = calculate_f1_score(precision_neg, recall_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "a1c30111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.62%\n",
      "Precision (Positive): 0.51\n",
      "Precision (Negative): 0.90\n",
      "Recall (Positive): 0.51\n",
      "Recall (Negative): 0.90\n",
      "F1-Score (Positive): 0.51\n",
      "F1-Score (Negative): 0.90\n",
      "Confusion Matrix:\n",
      "defaultdict(<class 'int'>, {('pos', 'pos'): 599, ('neg', 'pos'): 583, ('neg', 'neg'): 2689, ('pos', 'neg'): 311})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"Precision (Positive): {precision_pos:.2f}\")\n",
    "print(f\"Precision (Negative): {precision_neg:.2f}\")\n",
    "print(f\"Recall (Positive): {recall_pos:.2f}\")\n",
    "print(f\"Recall (Negative): {recall_neg:.2f}\")\n",
    "print(f\"F1-Score (Positive): {f1_pos:.2f}\")\n",
    "print(f\"F1-Score (Negative): {f1_neg:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "3e20be70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to C:\\Users\\ashri\\Documents\\AIT-526\\tweet\\ouputs\\model_performance.txt\n"
     ]
    }
   ],
   "source": [
    "# save the file :\n",
    "outputs = f\"\"\" \n",
    "model Performance: Navie Bayes\n",
    "Accuracy: {accuracy*100:.2f}%\n",
    "Precision (Positive): {precision_pos:.2f}\n",
    "Precision (Negative): {precision_neg:.2f}\n",
    "Recall (Positive): {recall_pos:.2f}\n",
    "Recall (Negative): {recall_neg:.2f}\n",
    "F1-Score (Positive): {f1_pos:.2f}\n",
    "F1-Score (Negative): {f1_neg:.2f}\n",
    "Confusion Matrix:{confusion_matrix}\n",
    "\"\"\"\n",
    "\n",
    "# Replace 'your_directory_path' with your actual directory path and choose a suitable filename\n",
    "filename = \"C:\\\\Users\\\\ashri\\\\Documents\\\\AIT-526\\\\tweet\\\\ouputs\\\\model_performance.txt\"\n",
    "\n",
    "# Write to the file\n",
    "with open(filename, \"w\") as file:\n",
    "    file.write(output_content)\n",
    "\n",
    "print(f\"Output saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31243ad6",
   "metadata": {},
   "source": [
    "#### Bonus point: \n",
    "how would the results change if you used term frequency x inverse document frequency instead of binary representation for Na√Øve Bayes?  \n",
    "How do your results change if you regularize your logistic regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f7bac459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "ffea882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf idf\n",
    "tf_idf = TfidfVectorizer()\n",
    "#applying tf idf to training data\n",
    "X_train_tf = tf_idf.fit_transform(train_data)\n",
    "X_train_tf = tf_idf.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d9963aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 4181, n_features: 7132\n"
     ]
    }
   ],
   "source": [
    "print(\"n_samples: %d, n_features: %d\" % X_train_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "6a2e495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 4182, n_features: 7132\n"
     ]
    }
   ],
   "source": [
    "X_test_tf = tf_idf.transform(test_data)\n",
    "\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "557848b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes classifier\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_tf, train_labels)\n",
    "y_pred = naive_bayes_classifier.predict(X_test_tf) # predicted Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e42f087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.78      1.00      0.87      3000\n",
      "    Negative       0.99      0.27      0.42      1182\n",
      "\n",
      "    accuracy                           0.79      4182\n",
      "   macro avg       0.88      0.63      0.65      4182\n",
      "weighted avg       0.84      0.79      0.75      4182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(test_labels, y_pred, target_names=['Positive', 'Negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2507d1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[2997    3]\n",
      " [ 864  318]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5b867be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a Logistic Regression model with L2 regularization\n",
    "model_l2 = LogisticRegression(penalty='l2', C=1.0)  # C is the inverse of regularization strength\n",
    "\n",
    "# With L1 regularization\n",
    "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "\n",
    "# Train the model using the same training data as above\n",
    "model_l2.fit(X_train_tf,train_labels)\n",
    "model_l1.fit(X_train_tf, train_labels)\n",
    "\n",
    "# Assuming X_train_tfidf is your TF-IDF transformed training data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "667ca0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with L2 Regularization Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.88      0.98      0.92      3000\n",
      "    Negative       0.93      0.65      0.76      1182\n",
      "\n",
      "    accuracy                           0.89      4182\n",
      "   macro avg       0.90      0.81      0.84      4182\n",
      "weighted avg       0.89      0.89      0.88      4182\n",
      "\n",
      "Logistic Regression with L1 Regularization Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.88      0.96      0.92      3000\n",
      "    Negative       0.86      0.68      0.76      1182\n",
      "\n",
      "    accuracy                           0.88      4182\n",
      "   macro avg       0.87      0.82      0.84      4182\n",
      "weighted avg       0.88      0.88      0.87      4182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Transform the test data with the TF-IDF vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "#predict the labels using ridge and lasso regression such that it will minimize both loss term and Regularization term\n",
    "y_pred_l2 = model_l2.predict(X_test_tfidf)\n",
    "y_pred_l1 = model_l1.predict(X_test_tfidf)\n",
    "\n",
    "# Print the classification reports for L1 and L2 Regularized Logistic Regression\n",
    "print(\"Logistic Regression with L2 Regularization Classification Report:\")\n",
    "print(classification_report(test_labels, y_pred_l2, target_names=['Positive', 'Negative']))\n",
    "\n",
    "print(\"Logistic Regression with L1 Regularization Classification Report:\")\n",
    "print(classification_report(test_labels, y_pred_l1, target_names=['Positive', 'Negative']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b20c7",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "So, to summarize this project,right after loading the data, it involves preprocessing the data, cleaning it, calculating frequencies of words individually, and then using those frequencies to compute the likelihoods, prior probabilities to make predictions. \n",
    "\n",
    "After predictions are made we assesed model performance, by calculating confusion matrix and printing classification report.\n",
    "\n",
    "Finally, we got an accuracy of 78% with overall presicion, recall, and F-1 score for positive(51%), negative(90%).\n",
    "\n",
    "But, on the other hand while doing it with Term frequency, IDF anf Logistic regression we found that model was balanced more then above report by accuracy of 79% with TF-IDF representation\n",
    "and L2(ridge regression) regularization imporoved accuracy to 89%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
